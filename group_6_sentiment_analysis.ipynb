{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __PROG8245 FINAL PROJECT - SENTIMENT ANALYSIS__\n",
    "### __GROUP:__ 6\n",
    "### __TEAM MEMBERS:__\n",
    "_Praiselin Lydia Gladston_\n",
    "\n",
    "_Sudharsan Tirumal_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __IMPORT REQURIED LIBRARIES:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer as SIA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\prais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prais\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. DATA COLLECTION:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Data collection from Reddit:__\n",
    "\n",
    "We have created a reddit account and using that user_agent, we will get raw data from sub-reddits that are relevant to our project.\n",
    "\n",
    "Authenticating with Reddit API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='XLj-2WpKbNdfpyJXVsCf_g',\n",
    "                     client_secret='0R4TzS0s-CPg16z92MPL1IyraTPjBg',\n",
    "                     user_agent='East-Code-7342')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentiment analysis system helps businesses improve their product offerings by learning what works and what doesn't. We will go through subreddits that can provide insights on comments that are useful for a business to understand recent trend. From the posts from relevant subreddits, we will collect their titles and perform sentiment analysis on them. Proceeding to collect all the titles into a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of related subreddits\n",
    "subreddit_list = ['PoliticsPeopleTwitter', 'trendingsubreddits', 'Discussion', 'CasualConversation']\n",
    "\n",
    "# List of categories to read\n",
    "relevant_categories = ['new', 'hot', 'top', 'controversial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the current trend, we will have to analyze the most recent posts. To achieve that, we will have analyze posts that fall only in that category. We will have to sort the collected data based on the date posted or how popular the content is.\n",
    "\n",
    "Function to get post titles from reddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_titles = set()\n",
    "\n",
    "for subreddit in subreddit_list:\n",
    "    for category in relevant_categories:\n",
    "        # Get subreddit instance\n",
    "        subreddit_instance = reddit.subreddit(subreddit)\n",
    "        \n",
    "        # Get posts from the specified category\n",
    "        if category == 'new':\n",
    "            posts = subreddit_instance.new(limit=2500)\n",
    "        elif category == 'hot':\n",
    "            posts = subreddit_instance.hot(limit=2500)\n",
    "        elif category == 'top':\n",
    "            posts = subreddit_instance.top(limit=2500)\n",
    "        elif category == 'controversial':\n",
    "            posts = subreddit_instance.controversial(limit=2500)\n",
    "        \n",
    "        # Extract post titles\n",
    "        for post in posts:\n",
    "            post_titles.add(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total post titles collected: 9732\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total post titles collected: {len(post_titles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to lowercase\n",
    "titles_in_lowercase = set()\n",
    "\n",
    "for item in post_titles:\n",
    "    titles_in_lowercase.add(item.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special characters removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of titles corrected: 7821\n"
     ]
    }
   ],
   "source": [
    "# Function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "# Clean the titles and create a new set\n",
    "cleaned_titles = {remove_special_characters(item) for item in titles_in_lowercase}\n",
    "\n",
    "# Counting the number of titles with special characters\n",
    "count_special = sum(bool(re.search(r'[^a-zA-Z0-9\\s]', item)) for item in titles_in_lowercase)\n",
    "\n",
    "print(f\"Number of titles corrected: {count_special}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Null entries removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing null entries\n",
    "cleaned_titles = set(filter(None, cleaned_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Data annotation:__\n",
    "\n",
    "As per requirement, data should be distinguished into atleast 3 classes. We will have the classes 'positive', 'negative' and 'neutral'. We're using Natural Language Toolkit library's SentimentIntensityAnalyzer for annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('trending subreddits for 20141022 rinstantregret rexcel rclimbing rnorwaypics rcatsridingroombas', 'neutral')\n",
      "('is it just me or is the japanese culture very unsettling specifically their urban legends and folklore', 'neutral')\n",
      "('why do so many good threads get downvoted on reddit', 'positive')\n",
      "('fine ill do it myself', 'negative')\n",
      "('muriqa', 'neutral')\n"
     ]
    }
   ],
   "source": [
    "sia = SIA()\n",
    "annotated_titles = []\n",
    "for title in cleaned_titles:\n",
    "    score = sia.polarity_scores(title)\n",
    "    compound = score['compound']\n",
    "    \n",
    "    if compound >= 0.05:\n",
    "        sentiment = 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "\n",
    "    annotated_titles.append((title, sentiment))\n",
    "\n",
    "# Print a sample of annotated titles\n",
    "for annotation in annotated_titles[:5]:\n",
    "    print(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting annotated data into DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trending subreddits for 20141022 rinstantregre...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is it just me or is the japanese culture very ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why do so many good threads get downvoted on r...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine ill do it myself</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muriqa</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Category\n",
       "0  trending subreddits for 20141022 rinstantregre...   neutral\n",
       "1  is it just me or is the japanese culture very ...   neutral\n",
       "2  why do so many good threads get downvoted on r...  positive\n",
       "3                              fine ill do it myself  negative\n",
       "4                                             muriqa   neutral"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert into DataFrame\n",
    "posts_df = pd.DataFrame.from_records(annotated_titles, columns=['Title', 'Category'])\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9684, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The titles have been collected into a csv with 2 columns 'Title' and 'Category'. Where 'Title' is the text content and 'Category' is the class the text falls into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. PREPROCESSING:__\n",
    "\n",
    "Preprocessing the text by removing hashtags, emojis, slang, stop-words, stemming/lemmatization, tokenizing and lowercasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove '#' from hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = emoji.demojize(text)\n",
    "\n",
    "    # Tokenizing the words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing non-alpha characters\n",
    "    tokens = [word for word in tokens if word.isalpha()] \n",
    "\n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))    \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Stemming and Lemmatizing the words\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(stemmer.stem(word)) for word in tokens] \n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Category</th>\n",
       "      <th>ProcessedTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trending subreddits for 20141022 rinstantregre...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>trend subreddit rinstantregret rexcel rclimb r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is it just me or is the japanese culture very ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>japanes cultur unsettl specif urban legend fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why do so many good threads get downvoted on r...</td>\n",
       "      <td>positive</td>\n",
       "      <td>mani good thread get downvot reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine ill do it myself</td>\n",
       "      <td>negative</td>\n",
       "      <td>fine ill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muriqa</td>\n",
       "      <td>neutral</td>\n",
       "      <td>muriqa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  Category   \n",
       "0  trending subreddits for 20141022 rinstantregre...   neutral  \\\n",
       "1  is it just me or is the japanese culture very ...   neutral   \n",
       "2  why do so many good threads get downvoted on r...  positive   \n",
       "3                              fine ill do it myself  negative   \n",
       "4                                             muriqa   neutral   \n",
       "\n",
       "                                      ProcessedTitle  \n",
       "0  trend subreddit rinstantregret rexcel rclimb r...  \n",
       "1  japanes cultur unsettl specif urban legend fol...  \n",
       "2                mani good thread get downvot reddit  \n",
       "3                                           fine ill  \n",
       "4                                             muriqa  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.loc[:, 'ProcessedTitle'] = posts_df['Title'].apply(preprocess_text)\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. FEATURE EXTRACTION:__ \n",
    "\n",
    "We will explore Bag-of-Words, GloVe and GPT-2 feature extraction techniques and how well they perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.65      0.70       477\n",
      "     neutral       0.82      0.93      0.87       979\n",
      "    positive       0.79      0.70      0.74       481\n",
      "\n",
      "    accuracy                           0.80      1937\n",
      "   macro avg       0.79      0.76      0.77      1937\n",
      "weighted avg       0.80      0.80      0.80      1937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=2000) \n",
    "\n",
    "# Fit and transform the text data to BoW vectors\n",
    "bow_features = vectorizer.fit_transform(posts_df['ProcessedTitle'])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_bow, X_test_bow, y_train_bow, y_test_bow = train_test_split(bow_features, posts_df['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifier (e.g., Logistic Regression)\n",
    "classifier_bow = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "classifier_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bow = classifier_bow.predict(X_test_bow)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Bag-of-Words Performance:\")\n",
    "print(classification_report(y_test_bow, y_pred_bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.81      0.05      0.10       477\n",
      "     neutral       0.52      0.99      0.68       979\n",
      "    positive       0.79      0.10      0.18       481\n",
      "\n",
      "    accuracy                           0.54      1937\n",
      "   macro avg       0.71      0.38      0.32      1937\n",
      "weighted avg       0.66      0.54      0.42      1937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe model (choose an appropriate model)\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")  # For example\n",
    "\n",
    "# Function to create document vectors\n",
    "def document_vector_glove(doc):\n",
    "    words = doc.split()\n",
    "    word_vectors = [glove_model[word] for word in words if word in glove_model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(glove_model.vector_size)  # Return a zero vector if no words are found\n",
    "    else:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "\n",
    "glove_features = np.array([document_vector_glove(doc) for doc in posts_df['ProcessedTitle']])\n",
    "\n",
    "X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(glove_features, posts_df['Category'], test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_glove_scaled = scaler.fit_transform(X_train_glove)\n",
    "X_test_glove_scaled = scaler.transform(X_test_glove)\n",
    "\n",
    "classifier_glove = LogisticRegression(max_iter=1000)\n",
    "classifier_glove.fit(X_train_glove_scaled, y_train_glove)\n",
    "y_pred_glove = classifier_glove.predict(X_test_glove)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"GloVe Performance:\")\n",
    "print(classification_report(y_test_glove, y_pred_glove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prais\\Workspace\\MLFoundations\\CSCN8010-class-notebooks\\venv\\CSCN8010_classic_ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.33      0.37       477\n",
      "     neutral       0.63      0.74      0.68       979\n",
      "    positive       0.44      0.37      0.40       481\n",
      "\n",
      "    accuracy                           0.55      1937\n",
      "   macro avg       0.49      0.48      0.48      1937\n",
      "weighted avg       0.53      0.55      0.53      1937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Load pre-trained GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Function to create document vectors using GPT embeddings\n",
    "def document_vector_gpt(doc):\n",
    "    if not doc.strip():\n",
    "        return np.zeros((1, model.config.hidden_size))  # Return a zero vector\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(doc, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract embeddings from the last hidden state (CLS token)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Extract GPT embeddings for each document (title)\n",
    "gpt_features = np.array([document_vector_gpt(doc) for doc in posts_df['ProcessedTitle']])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_gpt, X_test_gpt, y_train_gpt, y_test_gpt = train_test_split(gpt_features, posts_df['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a classifier on the GPT features\n",
    "classifier_gpt = LogisticRegression(max_iter=5000)\n",
    "classifier_gpt.fit(X_train_gpt.reshape((X_train_gpt.shape[0], -1)), y_train_gpt)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gpt = classifier_gpt.predict(X_test_gpt.reshape((X_test_gpt.shape[0], -1)))\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"GPT-2 Performance:\")\n",
    "print(classification_report(y_test_gpt, y_pred_gpt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation compared the performance of three feature extraction techniques, Bag-of-Words (BoW), GloVe, and GPT-2, for sentiment analysis of Reddit post headlines. BoW demonstrated strong overall performance, achieving an accuracy of 81% with high precision, recall, and F1-scores across all sentiment categories. In contrast, GloVe exhibited relatively poor performance, struggling with precision and recall for negative and positive sentiments, resulting in an accuracy of 52%. GPT-2 showed moderate performance with an accuracy of 54%, indicating potential but requiring further refinement for effective sentiment analysis. Overall, the evaluation underscores the effectiveness of BoW for sentiment analysis tasks based on Reddit post headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Category</th>\n",
       "      <th>ProcessedTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trending subreddits for 20141022 rinstantregre...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>trend subreddit rinstantregret rexcel rclimb r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is it just me or is the japanese culture very ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>japanes cultur unsettl specif urban legend fol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>why do so many good threads get downvoted on r...</td>\n",
       "      <td>positive</td>\n",
       "      <td>mani good thread get downvot reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fine ill do it myself</td>\n",
       "      <td>negative</td>\n",
       "      <td>fine ill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>muriqa</td>\n",
       "      <td>neutral</td>\n",
       "      <td>muriqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9679</th>\n",
       "      <td>steve bannon and roger stone are his friends n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>steve bannon roger stone friend need say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9680</th>\n",
       "      <td>hi i am writing in hopes someone can help me w...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hi write hope someon help ga get cardiologist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9681</th>\n",
       "      <td>trending subreddits for 20160811 rapocalympics...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>trend subreddit rapocalympicsrio rlastimag rno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9682</th>\n",
       "      <td>seriously this treasonous piece of shit can ju...</td>\n",
       "      <td>negative</td>\n",
       "      <td>serious treason piec shit go fuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9683</th>\n",
       "      <td>i just called my grandma and when she answered...</td>\n",
       "      <td>negative</td>\n",
       "      <td>call grandma answer first thing said well isnt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9684 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  Category   \n",
       "0     trending subreddits for 20141022 rinstantregre...   neutral  \\\n",
       "1     is it just me or is the japanese culture very ...   neutral   \n",
       "2     why do so many good threads get downvoted on r...  positive   \n",
       "3                                 fine ill do it myself  negative   \n",
       "4                                                muriqa   neutral   \n",
       "...                                                 ...       ...   \n",
       "9679  steve bannon and roger stone are his friends n...  positive   \n",
       "9680  hi i am writing in hopes someone can help me w...   neutral   \n",
       "9681  trending subreddits for 20160811 rapocalympics...   neutral   \n",
       "9682  seriously this treasonous piece of shit can ju...  negative   \n",
       "9683  i just called my grandma and when she answered...  negative   \n",
       "\n",
       "                                         ProcessedTitle  \n",
       "0     trend subreddit rinstantregret rexcel rclimb r...  \n",
       "1     japanes cultur unsettl specif urban legend fol...  \n",
       "2                   mani good thread get downvot reddit  \n",
       "3                                              fine ill  \n",
       "4                                                muriqa  \n",
       "...                                                 ...  \n",
       "9679           steve bannon roger stone friend need say  \n",
       "9680  hi write hope someon help ga get cardiologist ...  \n",
       "9681  trend subreddit rapocalympicsrio rlastimag rno...  \n",
       "9682                  serious treason piec shit go fuck  \n",
       "9683  call grandma answer first thing said well isnt...  \n",
       "\n",
       "[9684 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __4. MODEL TRAINING:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive-Bayes:\n",
    "\n",
    "Reason: Naive Bayes models are probabilistic classifiers based on Bayes' theorem. They are simple and efficient, making them suitable for text classification tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.72      0.65       477\n",
      "     neutral       0.93      0.65      0.77       979\n",
      "    positive       0.59      0.82      0.69       481\n",
      "\n",
      "    accuracy                           0.71      1937\n",
      "   macro avg       0.70      0.73      0.70      1937\n",
      "weighted avg       0.76      0.71      0.72      1937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_features, posts_df['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Naive Bayes Classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "print(\"Naive Bayes Classifier Performance:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM (Support Vector Machines):\n",
    "\n",
    "Reason: SVMs are powerful supervised learning models used for classification tasks, including sentiment analysis. They work well with high-dimensional data and can effectively separate data points in complex feature spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.68      0.72       477\n",
      "     neutral       0.86      0.93      0.89       979\n",
      "    positive       0.82      0.77      0.79       481\n",
      "\n",
      "    accuracy                           0.83      1937\n",
      "   macro avg       0.81      0.79      0.80      1937\n",
      "weighted avg       0.83      0.83      0.83      1937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the SVM Classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "print(\"SVM Classifier Performance:\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks:\n",
    "\n",
    "Deep learning models, such as recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformers, have gained popularity in sentiment analysis due to their ability to capture complex relationships in text data. These models can learn hierarchical representations of text, leading to improved sentiment classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prais\\Workspace\\MLFoundations\\CSCN8010-class-notebooks\\venv\\CSCN8010_classic_ml\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.5702 - loss: 0.8857\n",
      "Epoch 2/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8634 - loss: 0.3992\n",
      "Epoch 3/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9265 - loss: 0.2399\n",
      "Epoch 4/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9553 - loss: 0.1524\n",
      "Epoch 5/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9698 - loss: 0.1052\n",
      "Epoch 6/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9763 - loss: 0.0770\n",
      "Epoch 7/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9849 - loss: 0.0526\n",
      "Epoch 8/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 972us/step - accuracy: 0.9875 - loss: 0.0439\n",
      "Epoch 9/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9886 - loss: 0.0403\n",
      "Epoch 10/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9856 - loss: 0.0361\n",
      "Epoch 11/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9900 - loss: 0.0313\n",
      "Epoch 12/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9872 - loss: 0.0373\n",
      "Epoch 13/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9894 - loss: 0.0328\n",
      "Epoch 14/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9896 - loss: 0.0289\n",
      "Epoch 15/15\n",
      "\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9885 - loss: 0.0320\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 532us/step - accuracy: 0.7773 - loss: 1.4293\n",
      "Neural Network Performance: Accuracy = 0.78\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to categorical\n",
    "y_train_cat = to_categorical(y_train.factorize()[0])\n",
    "y_test_cat = to_categorical(y_test.factorize()[0])\n",
    "\n",
    "# Neural Network Model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(y_train_cat.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train.toarray(), y_train_cat, epochs=15, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test.toarray(), y_test_cat)\n",
    "print(\"Neural Network Performance: Accuracy = {:.2f}\".format(accuracy))\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance evaluation of three sentiment analysis models, Naive Bayes, Support Vector Machine (SVM), and a neural network implemented using TensorFlow, reveals varying degrees of accuracy and effectiveness. Naive Bayes achieves an accuracy of 78%, demonstrating solid precision and recall across all sentiment categories. SVM outperforms Naive Bayes slightly, with an accuracy of 80%, showcasing strong precision and recall for all sentiments. However, the neural network model falls significantly short, with an accuracy of only 23%, indicating challenges in effectively capturing sentiment patterns in the data. Further refinement and optimization may be necessary to improve its performance. Overall, **SVM appears to be the most robust model** for sentiment analysis, followed closely by Naive Bayes, while the neural network requires additional adjustments to enhance its efficacy in classifying sentiments accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\tDeployment and Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and predict sentiment\n",
    "def predict_sentiment():\n",
    "    user_input = text_input.get(\"1.0\", \"end-1c\")  \n",
    "    processed_input = preprocess_text(user_input)  \n",
    "    vectorized_input = vectorizer.transform([processed_input])  \n",
    "    prediction = svm_classifier.predict(vectorized_input)  \n",
    "    result_label.config(text=\"Predicted Sentiment: \" + str(prediction[0]))  \n",
    "\n",
    "# Tkinter window\n",
    "root = tk.Tk()\n",
    "root.title(\"Group 6: Sentiment Analysis\")\n",
    "\n",
    "# Text input widget with placeholder text\n",
    "text_input = tk.Text(root, height=15, width=60)\n",
    "text_input.insert(\"1.0\", \"Enter post/sentence here\")\n",
    "text_input.pack()\n",
    "\n",
    "# Predict button\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=predict_sentiment)\n",
    "predict_button.pack()\n",
    "\n",
    "# Label to display the result\n",
    "result_label = tk.Label(root, text=\"Predicted Sentiment is: \")\n",
    "result_label.pack()\n",
    "\n",
    "# Run the application\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
